# -*- coding: utf-8 -*-
"""DeepLearning Assignment 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L_UYIFX8PkyySoKoFZH0jj-SOTthSPqj

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
"""

!pip install -q datasets transformers evaluate timm albumentations
!pip install accelerate -U

from huggingface_hub import notebook_login
notebook_login()

import json
with open('result.json') as f:
  cocodata = json.load(f)

huggingdata = []
# Iterate through the images
for image in cocodata['images']:
# Remove the image directory from the file name
  image['file_name'] = image['file_name'].split('/')[-1]
  image['image_id'] = image['id']
# Extend the image dict with bounding boxes and class labels
  image['objects'] = {'bbox': [], 'category': [], 'area': [], 'id': []}
# Iterate through the annotations (bounding boxes and labels)
  for annot in cocodata['annotations']:
# Check if the annotation matches the image
    if annot['image_id'] == image['id']:
# Add the annotation
      image['objects']['bbox'].append(annot['bbox'])
      image['objects']['category'].append(annot['category_id'])
      image['objects']['area'].append(annot['area'])
      image['objects']['id'].append(annot['id'])
# Append the image dict with annotations to the list
  huggingdata.append(image)

with open("metadata.jsonl", 'w') as f:
  for item in huggingdata:
    f.write(json.dumps(item) + "\n")

from datasets import load_dataset
candy_data = load_dataset('imagefolder', data_dir="data")

candy_data

id2label = {item['id']: item['name'] for item in cocodata['categories']}
label2id = {v: k for k, v in id2label.items()}

from transformers import AutoImageProcessor
checkpoint = "facebook/detr-resnet-50"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)

import albumentations
import numpy as np
import torch

transform = albumentations.Compose(
    [
        albumentations.Resize(480, 480),
        albumentations.HorizontalFlip(p=1.0),
        albumentations.RandomBrightnessContrast(p=1.0),
    ],
    bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
)

def formatted_anns(image_id, category, area, bbox):
    annotations = []
    for i in range(0, len(category)):
        new_ann = {
            "image_id": image_id,
            "category_id": category[i],
            "isCrowd": 0,
            "area": area[i],
            "bbox": list(bbox[i]),
        }
        annotations.append(new_ann)

    return annotations

# transforming a batch
def transform_aug_ann(examples):
    image_ids = examples["image_id"]
    images, bboxes, area, categories = [], [], [], []
    for image, objects in zip(examples["image"], examples["objects"]):
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = transform(image=image, bboxes=objects["bbox"], category=objects["category"])

        area.append(objects["area"])
        images.append(out["image"])
        bboxes.append(out["bboxes"])
        categories.append(out["category"])

    targets = [
        {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
    ]

    return image_processor(images=images, annotations=targets, return_tensors="pt")

candy_data = candy_data.with_transform(transform_aug_ann)

def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch

from transformers import AutoModelForObjectDetection

model = AutoModelForObjectDetection.from_pretrained(
    checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)

#Fine tuned hyper Parameters
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="detr-resnet-50_finetuned_cppe5",
    per_device_train_batch_size=8,
    num_train_epochs=1000,
    fp16=True,
    save_steps=200,
    logging_steps=50,
    learning_rate=2e-5,
    weight_decay=1e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=True,
)

#train the model
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=candy_data['train'],
    tokenizer=image_processor,
)

trainer.train()

#Save the model
trainer.save_model('candy_detector')

#mounting google drive
from google.colab import drive
drive.mount('/content/drive')

#Loading the image. Update the method as required
from PIL import Image, ImageDraw
image_path = '/content/drive/MyDrive/Object_Detection/cd_32.jpg' #Change the path as necessary

image = Image.open(image_path)

image.show()

#defining the candy_counter function
def candy_counter(image):
  image_processor = AutoImageProcessor.from_pretrained("/content/candy_detector")
  model = AutoModelForObjectDetection.from_pretrained("/content/candy_detector")
  counter = {'Moon':0, 'Insect':0,'Black_star':0, 'Grey_star':0, 'Unicorn_whole':0, 'Unicorn_head':0,'Owl':0, 'Cat':0 }

  with torch.no_grad():
      inputs = image_processor(images=image, return_tensors="pt")
      outputs = model(**inputs)
      target_sizes = torch.tensor([image.size[::-1]])
      results = image_processor.post_process_object_detection(outputs, threshold=0.23, target_sizes=target_sizes)[0]

  for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
      box = [round(i, 2) for i in box.tolist()]
      if model.config.id2label[label.item()] == 'Moon':
        counter['Moon'] = counter['Moon'] + 1
      elif model.config.id2label[label.item()] == 'Insect':
        counter['Insect'] = counter['Insect'] + 1
      elif model.config.id2label[label.item()] == 'Black_star':
        counter['Black_star'] = counter['Black_star'] + 1
      elif model.config.id2label[label.item()] == 'Grey_star':
        counter['Grey_star'] = counter['Grey_star'] + 1
      elif model.config.id2label[label.item()] == 'Unicorn_whole':
        counter['Unicorn_whole'] = counter['Unicorn_whole'] + 1
      elif model.config.id2label[label.item()] == 'Unicorn_head':
        counter['Unicorn_head'] = counter['Unicorn_head'] + 1
      elif model.config.id2label[label.item()] == 'Owl':
        counter['Owl'] = counter['Owl'] + 1
      else:
        counter['Cat'] = counter['Cat'] + 1
  return counter

#calling the candy_counter function
candy_counter(image)

#Plotting the predicted boundary boxes
image_processor = AutoImageProcessor.from_pretrained("/content/candy_detector")
model = AutoModelForObjectDetection.from_pretrained("/content/candy_detector")
counter = {'Moon':0, 'Insect':0,'Black_star':0, 'Grey_star':0, 'Unicorn_whole':0, 'Unicorn_head':0,'Owl':0, 'Cat':0 }

with torch.no_grad():
    inputs = image_processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    target_sizes = torch.tensor([image.size[::-1]])
    results = image_processor.post_process_object_detection(outputs, threshold=0.23, target_sizes=target_sizes)[0]

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
draw = ImageDraw.Draw(image)

for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
    box = [round(i, 2) for i in box.tolist()]
    x, y, x2, y2 = tuple(box)
    draw.rectangle((x, y, x2, y2), outline="red", width=1)
    draw.text((x, y), model.config.id2label[label.item()], fill="white")

image

